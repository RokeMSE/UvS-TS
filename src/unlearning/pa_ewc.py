# Calculates L_pop, MMD, and the Population-Aware FIM
import torch
from src.utils.statistic import (get_marginal_distribution, get_cross_correlations,
                                 get_power_spectral_density, get_autocorrelation)
from src.utils.losses import mmd_loss

def calculate_L_pop(model_output, ground_truth_data, weights=None):
    """
    Calculates the Generalized Population Loss (L_pop) as the weighted sum of MMDs over several key statistics

    Params:
        model_output (Tensor): Data generated by the model. Shape: (batch, seq_len, num_nodes).
        ground_truth_data (Tensor): Real data from the retain set. Shape: (batch, seq_len, num_nodes).
        weights (dict, optional): A dictionary of weights for each statistic.
                                  Defaults to equal weights of 1.0.

    Returns:
        torch.Tensor: The final L_pop loss value.
    """
    if weights is None:
        weights = { # default weights (all == 1.0 for simplicity)
            'marginal': 1.0,
            'acf': 1.0,
            'psd': 1.0,
            'cc': 1.0,
        }

    total_loss = 0.0

    # --- 1. Calculate statistics for both ground truth and model-generated data ---
    # NOTE: Calculating all of these for every batch can be very slow.
    # For initial implementation and testing, you might want to use only one or two,
    # like 'marginal' and 'acf', by adjusting the weights dictionary.

    # Marginal Value Distributions
    if weights.get('marginal', 0.0) > 0:
        gt_marginal = get_marginal_distribution(ground_truth_data)
        gen_marginal = get_marginal_distribution(model_output)
        loss_marginal = mmd_loss(gt_marginal, gen_marginal)
        total_loss += weights['marginal'] * loss_marginal

    # Autocorrelation Functions (ACF)
    if weights.get('acf', 0.0) > 0:
        gt_acf = get_autocorrelation(ground_truth_data)
        gen_acf = get_autocorrelation(model_output)
        if len(gt_acf) > 0 and len(gen_acf) > 0: # Ensure ACF calculation was possible
             loss_acf = mmd_loss(gt_acf, gen_acf)
             total_loss += weights['acf'] * loss_acf

    # Power Spectral Densities (PSD)
    if weights.get('psd', 0.0) > 0:
        gt_psd = get_power_spectral_density(ground_truth_data)
        gen_psd = get_power_spectral_density(model_output)
        loss_psd = mmd_loss(gt_psd, gen_psd)
        total_loss += weights['psd'] * loss_psd

    # Cross-Correlations (CC)
    if weights.get('cc', 0.0) > 0:
        gt_cc = get_cross_correlations(ground_truth_data)
        gen_cc = get_cross_correlations(model_output)
        if len(gt_cc) > 0 and len(gen_cc) > 0:
            loss_cc = mmd_loss(gt_cc, gen_cc)
            total_loss += weights['cc'] * loss_cc

    return total_loss

def calculate_pa_fim(model, retain_data_loader, device):
    """
    Calculates the diagonal of the Population-Aware Fisher Information Matrix (FIM)

    Params:
        model (nn.Module): The model (theta*) to calculate the FIM for.
        retain_data_loader (DataLoader): DataLoader for the retain set (D_r).
        device (torch.device): The device to perform calculations on (e.g., 'cuda').

    Returns:
        dict: A dictionary mapping parameter names to their FIM diagonal values.
    """
    model.eval()
    # Initialize a dictionary to store the FIM diagonal for each parameter.
    fim_diagonal = {name: torch.zeros_like(param) for name, param in model.named_parameters() if param.requires_grad}

    # Loop through the retain data to compute the expectation.
    for data_batch in retain_data_loader:
        # Assuming data_batch is a tensor or the first element of a tuple/list
        if isinstance(data_batch, (list, tuple)):
            data_batch = data_batch[0]
        data_batch = data_batch.to(device)

        model.zero_grad()

        # The model output is the generated data distribution Q_s(theta)
        model_output = model(data_batch)

        # Calculate L_pop, which is the loss function for our FIM
        loss = calculate_L_pop(model_output, data_batch)

        # Calculate the gradients of L_pop w.r.t. model parameters
        loss.backward()

        # Accumulate the squared gradients for each parameter
        for name, param in model.named_parameters():
            if param.grad is not None:
                fim_diagonal[name] += param.grad.data.pow(2)

    # Average the FIM diagonal over the number of samples in the dataset
    num_samples = len(retain_data_loader.dataset)
    for name in fim_diagonal:
        fim_diagonal[name] /= num_samples

    # NOTE: This is computationally and memory-intensive. For large models or datasets,
    # MIGHT need to APPROXIMATE this by using a smaller subset of D_r.
    return fim_diagonal